# B: Human Evaluation Setup

B. Human Evaluation Setup To ensure the reliability of our automated metrics and strict benchmarking of our method, this paper conducted two distinct human evaluation experiments. Both evaluations employed the same four dimensions deﬁned in Section 4(Faithfulness, Conciseness, Readability, and Aesthetics) and adhered to the same detailed rubrics used by our VLM judge. We utilized Streamlit to build dedicated annotation interfaces for these tasks. Validation of VLM-as-a-Judge. The objective of this human evaluation is to assess the alignment between our VLM-based judge (Gemini-3-Pro) and human judgment. We randomly sampled 50 cases (25 from the Vanilla baseline and 25 from PaperBanana) from the test set. For each case, two experienced researchers were presented with the Method Section, Caption, the human-drawn reference diagram, and a model-generated candidate (either from our method or the baseline). They were tasked with conducting a side-by-side comparison on the four evaluation dimensions. For conﬂicting cases, they engaged in discussion to reach a consensus. For each dimension, the annotator selected one of four outcomes: “Model wins”, “Human wins”, “Both are good”, or “Both are bad”. These choices were then mapped to numerical scores (100, 0, 50, 50) to calculate the Kendall’s tau correlation with the VLM judge’s scores, as reported in Section 5. The annotation interface is shown in Figure 11. Blind Test for Main Results. To rigorously compare PaperBanana against the strong baseline (Vanilla Nano-Banana-Pro), we conducted a blind A/B test on a subset of 50 cases. Three experienced researchers were presented with the Method Section, Caption, a Reference (Human Drawn) diagram, and two anonymous candidates (Candidate A and Candidate B) in randomized order. To determine the winner, we enforced a hierarchical decision strategy consistent with our VLM evaluation protocol. Annotators ﬁrst evaluated the Primary Dimensions (Faithfulness and Readability). If a candidate won in the primary dimensions (or won one and tied the other), it was declared the overall winner. In cases of a tie in primary dimensions, the decision was deferred to the Secondary Dimensions (Conciseness and Aesthetics). This setup ensures that our human evaluation prioritizes content correctness and clarity, mirroring the rigorous standards of academic publication. The annotation interface is shown in Figure 12. Figure 11 |Annotation interface for reference-based evaluation. 21



[[PAGE \1]]

PaperBanana: Automating Academic Illustration for AI Scientists Figure 12 |Annotation interface for blind human evaluation.
