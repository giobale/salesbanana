# 4. Benchmark Construction

4. Benchmark Construction The lack of benchmarks hinders rigorous evaluation of automated diagram generation. We address this with PaperBananaBench, a dedicated benchmark curated from NeurIPS 2025 methodology diagrams, capturing the sophisticated aesthetics and diverse logical compositions of modern AI papers. We detail the construction pipeline and evaluation protocol below; dataset statistics are in Figure 3. 4.1. Data Curation Collection & Parsing. We begin by randomly sampling 2,000 papers from the 5,275 publications at NeurIPS 2025 and retrieving their PDF Ô¨Åles. Subsequently, we utilize the MinerU toolkit (Niu et al.,2025) to parse these documents, extracting the text of the methodology sections, and all the diagrams and their captions in the paper. Figure 3 | [Generated by ] Statistics of the test set of PaperBananaBench (totaling 292 samples). The average length of source context / Ô¨Ågure caption is 3,020.1 / 70.4 words. Filtering. We then apply a Ô¨Åltering stage to ensure data quality. First, we discard papers without methodology diagrams, yielding 1,359 valid candidates. Second, we restrict the aspect ratio (ùë§:‚Ñé) to [1.5,2.5]. Ratios below 1.5 are excluded as methodology diagrams typically require wider landscape layouts for logical Ô¨Çows, while ratios exceeding 2.5 are unsupported by current image generation models. Including such outliers would introduce bias in side-byside evaluations by revealing the human origin of candidates. This yields 610 valid candidates, each a tuple (ùëÜ, ùêº, ùê∂) , where ùëÜ is the methodology description, ùêº is the methodology diagram, and ùê∂is the caption. Categorization. To facilitate future analysis of generating diÔ¨Äerent types of diagrams, we further 5



[[PAGE \1]]

PaperBanana: Automating Academic Illustration for AI Scientists categorize the diagrams into four classes, based on visual topology and content: Agent & Reasoning, Vision & Perception,Generative & Learning, and Science & Applications (see Appendix Cfor deÔ¨Ånitions). Gemini-3-Pro is used to perform the categorization, assigning samples with hybrid elements to their predominant category. Human Curation. Finally, we conduct a human curation phase to guarantee the integrity and quality of the dataset. Annotators are tasked with verifying and correcting the extracted methodology descriptions and captions, validating the correctness of diagram categorizations, and Ô¨Åltering out diagrams of insuÔ¨Écient visual quality (e.g., overly simplistic, cluttered, or abstract designs). Following this rigorous process, 584 valid samples remain. We randomly partition these into two equal subsets: a test set ( ùëÅ= 292) for evaluation and a reference set ( ùëÅ= 292) to facilitate retrieval-augmented in-context learning. 4.2. Evaluation Protocol We utilize VLM-as-a-Judge to assess the quality of methodology diagrams and statistical plots. Given the inherent subjectivity in evaluating visual design, we employ a referenced comparison approach where the judge compares the model-generated diagram against the human-drawn diagram to determine which better satisÔ¨Åes each evaluation criterion. Evaluation Dimensions. Inspired by Quispel et al. (2018), we evaluate diagrams on two perspectives. Detailed rubrics for each dimension are provided in Appendix H. ‚Ä¢ Content (Faithfulness & Conciseness): Faithfulness ensures alignment with the source context (methodology description) and communicative intent (caption), while Conciseness requires focusing on core information without visual clutter. ‚Ä¢ Presentation (Readability & Aesthetics): Readability demands intelligible layouts, legible text, no excessive crossing lines, etc. Aesthetics evaluates adherence to the stylistic norms of academic manuscripts. Referenced Scoring. For each dimension, the VLM judge compares the model-generated diagram against the human reference given the context and caption. It determines Model wins,Human wins, or Tie based on relative quality, which are then mapped to scores of 100, 0, and 50, respectively. To aggregate scores into an overall metric, we follow the design principle that information visualization must primarily ‚Äúshow the truth‚Äù (Mackinlay,1986;Quispel et al.,2018;Tufte and Graves-Morris, 1983). We employ a hierarchical aggregation strategy, designating faithfulness and readability as primary dimensions, and conciseness and aesthetics as secondary. If primary dimensions yield a decisive winner (i.e., winning both, or winning one with a tie), this determines the overall winner. In case of a tie (e.g., each wins one, or both tie), we apply the same rule to the secondary dimensions. This hierarchical approach ensures that content Ô¨Ådelity and clarity take precedence over aesthetics and conciseness.
