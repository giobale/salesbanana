# 5. Experiments

5. Experiments 5.1. Baseline Methods and Models We compare PaperBanana against three baseline settings: (1) Vanilla, directly prompting the image generation model to generate diagrams based on the input context (methodology description and caption); (2) Few-shot, building upon the vanilla baseline by augmenting the prompt with 10 few-shot examples, where each example consists of a triplet (methodology description, caption, diagram) to enable in-context learning for the image generation model; (3) Paper2Any (Liu et al.,2025), an agentic 6



[[PAGE \1]]

PaperBanana: Automating Academic Illustration for AI Scientists Table 1 |Main results on PaperBananaBench. Best score in each column is in bold. Method Faithfulness ↑Conciseness ↑Readability ↑Aesthetic ↑Overall ↑ Vanilla Settings GPT-Image-1.5 4.5 37.5 30.0 37.0 11.5 Nano-Banana-Pro 43.0 43.5 38.5 65.5 43.2 Few-shot Nano-Banana-Pro 41.6 49.6 37.6 60.5 41.8 Agentic Frameworks Paper2Any (w/ Nano-Banana-Pro) 6.5 44.0 20.5 40.0 8.5 PaperBanana (Ours) w/ GPT-Image-1.5 16.0 65.0 33.0 56.0 19.0 w/ Nano-Banana-Pro 45.8 80.7 51.4 72.1 60.2 Human 50.0 50.0 50.0 50.0 50.0 framework that generates diagrams to present high-level ideas of the papers, which is the closest to our setting. For VLM backbone, we default to Gemini-3-Pro, while for image generation model, we experiment with Nano-Banana-Pro and GPT-Image-1.5. (See Appendix Cfor more implementation details.) 5.2. Evaluation Settings. Evaluating the quality of generated diagrams demands strong visual perception and understanding capabilities, particularly for the Faithfulness dimension, which requires accurately identifying and interpreting subtle modules and connections. Hence, we employ Gemini-3-Pro as our VLM-based Judge. To validate its reliability, we randomly sampled 50 cases (25 from vanilla and 25 from our method) and conducted a two-fold validation process: Inter-Model Agreement (Consistency). First, we verify that our evaluation protocol is robust and model-agnostic. We evaluated the agreement between our judge (Gemini-3-Pro) and other distinct VLMs (Gemini-3-Flash and GPT-5). Kendall’s tau correlations with Gemini-3-Flash across the four dimensions (Faithfulness, Conciseness, Readability, Aesthetic) and their aggregation are 0.51, 0.60, 0.45, 0.56, and 0.55, respectively; correlations with GPT-5 are 0.43, 0.47, 0.44, 0.42, and 0.45, respectively. This conﬁrms the consistency of our protocol across diﬀerent judge models1. Human Alignment (Validity). Second, we verify that our VLM judge is a valid proxy for human evaluation. We tasked two human annotators to independently perform reference-based scoring on the same 50 samples using the same rubrics, followed by a discussion to reach consensus on conﬂicting cases. Kendall’s tau correlations between Gemini-3-Pro and human annotations are 0.43, 0.57, 0.45, 0.41, and 0.45, respectively. These strong correlations demonstrate that our VLM-based judge aligns well with human perception. (See Appendix Bfor more details.) 5.3. Main Results Table 1summarizes the performance of ours and baseline methods on PaperBananaBench.PaperBanana consistently outperforms leading baselines across all metrics. We attribute the poor performance of GPT-Image in both vanilla and agentic settings to its weaker instruction following and text rendering capabilities compared to Nano-Banana-Pro, which fails to meet the strict requirements 1 According to existing literatures (Cohen,2013;Hollander et al.,2013), a Kendall’s tau correlation exceeding 0.4 is generally considered to represent relatively strong agreement 7



[[PAGE \1]]

PaperBanana: Automating Academic Illustration for AI Scientists Table 2 | Ablation study on PaperBananaBench. The shaded row indicates the default setting of PaperBanana. We systematically ablate each agent component to assess its contribution. The  symbol denotes the Random Retriever which randomly selects 10 examples instead of performing semantic retrieval. # Module Faithfulness ↑Conciseness ↑Readability ↑Aesthetic ↑Overall ↑ Retriever Planner Stylist Visualizer Critic ①✓ ✓ ✓ ✓ 3 iters 45.8 80.7 51.4 72.1 60.2 ②✓ ✓ ✓ ✓ 1 iter 38.3 75.2 50.6 68.9 51.8 ③✓ ✓ ✓ ✓ - 30.7 79.2 47.0 72.1 45.6 ④✓ ✓ -✓- 39.2 61.7 47.9 67.4 49.2 ⑤✓-✓- 37.3 62.7 51.1 65.6 48.3 ⑥-✓-✓- 41.9 58.6 43.1 62.9 44.2 of academic illustration. Similarly, while Paper2Any also supports generating paper ﬁgures, it prioritizes the presentation of high-level ideas rather than the faithful depiction of speciﬁc methodological ﬂows necessary for methodology diagrams. This objective mismatch leads to its underperformance in our evaluation setting. In contrast, PaperBanana achieves comprehensive improvements over the Vanilla Nano-BananaPro baseline: Faithfulness (+2.8%), Conciseness (+37.2%), Readability (+12.9%), and Aesthetics (+6.6%), contributing to a +17.0% gain in the Overall score. Regarding performance across categories, Agent & Reasoning achieves the highest overall score (69.9%), followed by Scientiﬁc & Application (58.8%) and Generative & Learning (57.0%), while Vision & Perception scores the lowest (52.1%). We also conducted a blind human evaluation on a subset of 50 cases to compare PaperBanana against vanilla Nano-Banana-Pro (See Appendix Bfor details). The average win / tie / loss rate of PaperBanana from 3 human judges is 72.7% / 20.7% / 6.6%, respectively. This further validates that our agentic workﬂow shows promising improvements in automated methodology diagram generation. (See Appendix Figure 7for case studies) Despite the progress, we note that PaperBanana still underperforms the human reference in terms of faithfulness. We have included some failure analysis in Appendix Figure 10 to provide insights into the challenges. 5.4. Ablation Study To understand the contribution of each agent component, we conduct an ablation study, with results presented in Table 2. Impact of the Retriever Agent. We compare the semantic retriever with random and no-retriever baselines (rows ④ – ⑥ in Table 2). Without reference examples as guidance, the no-retriever setting signiﬁcantly underperforms in Conciseness, Readability, and Aesthetics, as the Planner defaults to verbose, exhaustive descriptions. Moreover, lacking exposure to academic diagram aesthetics, this setting produces visually less reﬁned outputs. Interestingly, the random retriever achieves performance comparable to the semantic approach, suggesting that providing general structural and stylistic patterns is more critical than precise content matching. Impact of the Stylist and Critic Agents. Comparing rows ③ and ④ shows that the Stylist boosts Conciseness (+17.5%) and Aesthetics (+4.7%) but lowers Faithfulness (-8.5%), as visual polishing sometimes omits technical details. However, the Critic Agent (row ① vs. ③ ) eﬀectively bridges this 8



[[PAGE \1]]

PaperBanana: Automating Academic Illustration for AI Scientists Figure 4 | [Generated by ] Vanilla Gemini-3-Pro vs. PaperBanana for statistical plots generation. Figure 5 | [Generated by ] Coding vs. Image Generation for visualizing statistical plots. gap, substantially recovering Faithfulness. Additional iterations further enhance all metrics, ensuring a balance between aesthetics and technical accuracy. 5.5. PaperBanana for Statistical Plots Generation. PaperBanana operates by ﬁrst synthesizing a detailed description of the target illustration, then visualizing it into an image. Unlike methodology diagrams that prioritize aesthetics and logical coherence, statistical plots demand rigorous numerical precision, making standard image generation models unsuitable. To address this, we demonstrate that by adopting executable code for visualization, PaperBanana seamlessly extends to statistical plot generation. Testset Curation. Following the task formulation in Section 2, we assess PaperBanana’s capability to generate statistical plots from tabular data and brief visual descriptions. Since raw data of statistical plots is rarely available in academic manuscripts, we repurpose ChartMimic (Yang et al.,2025b), a dataset originally constructed for chart-to-code generation. This dataset primarily includes statistical plots from arXiv papers and Matplotlib galleries, paired with human-curated Python code. Leveraging Gemini-3-Pro, we extract the underlying tabular data from the code and synthesize a brief description for each plot. Following rigorous ﬁltering and sampling (see Appendix D), we curate 240 test cases and 240 reference examples, stratiﬁed across seven plot categories—bar chart, line chart, tree & pie chart, scatter plot, heatmap, radar chart, and miscellaneous—and two complexity levels (easy and hard). For evaluation, we adhere to the protocol detailed in Section 4, with prompts speciﬁcally tailored to statistical plots. Figure 4compares PaperBanana with vanilla Gemini-3-Pro on our curated test set. Our method consistently outperforms the baseline across all dimensions, achieving gains of +1.4%, +5.0%, +3.1%, and +4.0% in Faithfulness, Conciseness, Readability, and Aesthetics, respectively, resulting in a +4.1% overall improvement. Notably, PaperBanana slightly surpasses human performance in Conciseness, Readability, and Aesthetics while remaining competitive in Faithfulness, showcasing its eﬀectiveness for statistical plot.
